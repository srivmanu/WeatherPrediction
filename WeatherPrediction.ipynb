{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WeatherPrediction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0AwEaChTCzN",
        "colab_type": "text"
      },
      "source": [
        "Link for original live copy is here : \n",
        "[Google Colab link (with comment enabled)](https://colab.research.google.com/drive/1VcIU5kY3YHIIWShgLgywwjfkJa9XW6qZ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWE6-PaFIZQS",
        "colab_type": "text"
      },
      "source": [
        "Imports necessary packages.\n",
        "\n",
        "\n",
        "1.   Basically numpy is a math and matrix based package\n",
        "2.   pandas is a large dataset management package\n",
        "3.   keras is a neural network library\n",
        "4.   sklearn is a datascience based package\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_PhWfIecc-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import array\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from pandas import to_datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras import callbacks\n",
        "from keras import losses\n",
        "from keras import activations\n",
        "from keras.layers import Activation\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNLj3cJspHsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9L-A4OAEvEI",
        "colab_type": "code",
        "outputId": "29e4cf11-6f8b-4206-dc91-88c0c9e76239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "#   f.write('Hello Google Drive!')\n",
        "# !cat '/gdrive/My Drive/foo.txt'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UOtuuuarG6F4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwjcZez5Iuu2",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "Functions to organize and save training data in chunks of 'n', and get testing output data based on those chunks (n+1th row)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mfXuWn4m5mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTrain(data,chunk):\n",
        "    train = []\n",
        "    for dataVal in range(len(data) - chunk):\n",
        "        train.append(data[dataVal:dataVal+chunk])\n",
        "    return array(train)\n",
        "\n",
        "def getTest(data,chunk):\n",
        "    test = []\n",
        "    for dataVal in range(chunk,len(data)):\n",
        "        test.append(data[dataVal])\n",
        "    return array(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UviVv6aIYA0",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "Functions to import data. Data is input from 'Basel_Data.csv'. Which is then stored into variable named 'df' short for data-frame\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0TxYsCod4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDateFromData(rowVal):\n",
        "    strVal = str(int(rowVal['Year'])) + \"-\"  + str(int(rowVal['Month']))  +\"-\"  + str(int(rowVal['Day'])) + \" \"  + str(int(rowVal['Hour'])) + \":\" + str(int(rowVal['Minute'])) + \":00\"\n",
        "    return to_datetime(strVal)\n",
        "\n",
        "def convertToDateTime(file):\n",
        "  todelete = ['Year','Month','Day','Hour','Minute']\n",
        "  file['DateTime'] = file.apply(lambda row:getDateFromData(row), axis =1)\n",
        "  cols = file.columns.tolist()\n",
        "  cols = cols[-1:] + cols[:-1]\n",
        "  file = file[cols]\n",
        "  return file.drop(todelete, axis=1)\n",
        "\n",
        "def importFile():\n",
        "  return read_csv(\"/gdrive/My Drive/MLProj/Basel_Data.csv\")\n",
        "\n",
        "def fixColumns(df):\n",
        "  df.columns = [\n",
        "    'DateTime',\n",
        "    'temperature_2m',\n",
        "    'relative_humidity_2m',\n",
        "    'mean_sea_level_pressure',\n",
        "    'total_precipitation_highres_sfc',\n",
        "    'total_precipitation_lowres_sfc',\n",
        "    'snowfall_highres_sfc',\n",
        "    'snowfall_lowres_sfc',\n",
        "    'total_cloud_cover_sfc',\n",
        "    'high_cloud_cover_sfc',\n",
        "    'medium_cloud_cover_sfc',\n",
        "    'low_cloud_cover_sfc',\n",
        "    'sunshine_duration_sfc',\n",
        "    'shortwave_duration_sfc',\n",
        "    'wind_speed_10m',\n",
        "    'wind_direction_10m',\n",
        "    'wind_speed_80m',\n",
        "    'wind_direction_80m',\n",
        "    'wind_speed_900mb',\n",
        "    'wind_direction_900mb',\n",
        "    'wind_gust_sfc'\n",
        "  ]\n",
        "  return df\n",
        "\n",
        "df = importFile()\n",
        "df = convertToDateTime(df)\n",
        "df = fixColumns(df)\n",
        "df.fillna(0,inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZH795N8JMJA",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "checking if data input is correct and without issues.\n",
        "\n",
        "line 2 removes dateTime column from input data, as it will not affect the training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "457g8ziisUIP",
        "colab_type": "code",
        "outputId": "0bf09d8a-67f0-42c2-e142-64aeb41b4890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "dataset = df\n",
        "values = dataset.values[:,1:]\n",
        "print(values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99 96 1006.5 ... 20.46 265.42 11.9]\n",
            " [1.0 97 1006.0 ... 20.18 264.72 11.9]\n",
            " [1.05 97 1005.3 ... 18.86 263.89 11.9]\n",
            " ...\n",
            " [11.16 66 1018.4 ... 11.5 89.5 5.8]\n",
            " [10.72 66 1018.6 ... 12.71 92.7 4.4]\n",
            " [10.33 68 1018.8 ... 13.99 96.57 5.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvoptLwGJZ9X",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "chunk is the number of values to consider as input to attain 1 row of output. Currently assuming 3 days, i.e. 72 hours as input\n",
        "\n",
        "spilting training and validation data based on 30 years worth data. (line 3-9)\n",
        "\n",
        "Line 11 checks the data format and its dimensions\n",
        "\n",
        "\n",
        "Here we have data input as an array of chunks 'n' of data, each succeeding the previous by 1. That way, we have 'n' data input that solves for the n+1th data. The n+1th data is found in the output value 'y'\n",
        "\n",
        "To Explore : Implement based on hourly+daily+weekly+fortnightly+monthly+quarterly+yearly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nr8OURWAb9Z",
        "colab_type": "code",
        "outputId": "d28d3a33-1fbe-4f81-ace1-4b1f5b491a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chunk_size = 96\n",
        "# Make chunks in parts. \n",
        "n_train_hours = 365 * 24 * 20\n",
        "\n",
        "\n",
        "train = values[:n_train_hours, :]\n",
        "test = values[n_train_hours:, :]\n",
        "\n",
        "(train_X,train_y,test_X,test_y) = (getTrain(train,chunk_size) , getTest(train,chunk_size) , getTrain(test,chunk_size) , getTest(test,chunk_size))\n",
        "test_y = test_y[:,0]\n",
        "train_y = train_y[:,0] \n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
        "# train_X = train_X[:,:,[0,1,2,4,11,19]]\n",
        "# test_X = test_X[:,:,[0,1,2,4,11,19]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175104, 96, 20) (175104,) (124896, 96, 20) (124896,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLJquqZM3oV",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "The following code makes it so that the model training data is stored at every epoch, so that if model is interrupted, it can start at the same instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brWOIA9tMjYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/gdrive/My Drive/MLProj/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create checkpoint callback\n",
        "cp_callback = callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4AjvmczOdX",
        "colab_type": "text"
      },
      "source": [
        "#For following code \n",
        "\n",
        "[LSTM or Long-Short term memory ](https://en.wikipedia.org/wiki/Long_short-term_memory) based Neural network. Considers chunks of data as input for 1 output (kind of fits my data model). It is a form of Recurrent Neural network. \n",
        "\n",
        "\n",
        "Line 1-4 define the model \n",
        "Line  6 runs it on input data train_X, expecting output train_y. Runs for 100 iterations on all data in train_X. utilizes batch size of 72, meaning it calculates 72 data rows at once (72 X 4 row input => 1 row output). \n",
        "It considers test_X and test_y data as validation dataset, and evaluates it accuracy on them as well for each iteration.\n",
        "\n",
        "Line 8 extracts the loss obtained in training the data (error of model's inherent weights and parameters with number of epochs, as we can see it gets more accurate with more iterations.) . Creating a graph of iteration vs training loss\n",
        "\n",
        "\n",
        "Line 9 does the same for validation loss\n",
        "\n",
        "Line 10-11 prints the plots of loss vs epochs\n",
        "\n",
        "Line 13 tests the shape of test data (just because i was worried I was mucking up the data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr9igl5f5VYM",
        "colab_type": "code",
        "outputId": "0354d90b-5b1f-4a42-ad62-48d55530d7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "input_shape=(train_X.shape[1],train_X.shape[2])\n",
        "print(input_shape)\n",
        "model = Sequential()\n",
        "model.add(LSTM(400, input_shape=input_shape, use_bias=True))\n",
        "# model.add(LSTM(20, use_bias=True, return_sequences=True ))\n",
        "model.add(Dense(20))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(activations.linear))\n",
        "model.compile(loss=losses.mse, optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 20)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 400)               673600    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 20)                8020      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 21        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 681,641\n",
            "Trainable params: 681,641\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ijJVk1NntTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model, to_file='/gdrive/My Drive/MLProj/model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am4deKxDIgwh",
        "colab_type": "text"
      },
      "source": [
        "The following code should load previous model from gdrive and run it forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jV-yMobesYa",
        "colab_type": "code",
        "outputId": "e759fabb-552d-4950-f32f-fc264a71ff67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "print(model.predict(test_X[:5]))\n",
        "print(test_y[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00927737]\n",
            " [1.6255051 ]\n",
            " [0.51656115]\n",
            " [1.7935371 ]\n",
            " [1.380176  ]]\n",
            "[1.09 1.24 1.36 1.51 1.67]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8pDNwznIDV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_X, train_y, epochs=25, batch_size=256, validation_data=(test_X,test_y), verbose=1, shuffle=False,callbacks=[cp_callback])\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3MLtrmLLdnp",
        "colab_type": "text"
      },
      "source": [
        "#For the following code\n",
        "We use the trained model to predict temperature and look at the predicted result\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biGwIyoHqaty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat = model.predict(test_X[:10000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_HuPcz_kUSq",
        "colab_type": "code",
        "outputId": "dd1df6bf-8aa6-42c4-db0c-c86d8fea0f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # calculate RMSE\n",
        "rmse = sqrt(mean_squared_error(test_y[:10000], yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 1.699\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}